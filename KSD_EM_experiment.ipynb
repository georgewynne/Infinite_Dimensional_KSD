{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm_ntb\n",
    "from scipy.stats import multinomial\n",
    "from functools import partial\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import scipy.interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KSD Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KSD class if the main object used in the tests.\n",
    "\n",
    "It is calculated using a finite dimensional approximation of the data using the basis corresponding to the base Gaussian measure.\n",
    "\n",
    "For example, if we are considering functions over L^{2}([0,1]) and the base Gaussian measure is a Brownian motion. Then the basis is the standard Brownian basis e_{n}(t) = \\sqrt{2}\\sin((n-0.5)\\pi t)\n",
    "\n",
    "We approximate L^{2}([0,1]) inner products by <f,g> = \\sum_{n=1}^{\\infty}<f,e_{n}><g,e_{n}> \\approx \\sum_{n=1}^{n_freqs}<f,e_{n}><g,e_{n}> for some value n_freqs standing for number of frequencies.\n",
    "\n",
    "So the data one inputs into KSD is the n_freqs coefficients with respect to the Brownian motion basis.\n",
    "\n",
    "This is different from the projections used in functional data analysis which aim to use a low number of frequencies to represent signals, often no more than 12-15. Whereas we use a high number, n_freqs = 100 in our experiments, as it is purely a technique to make inner products easier to compute rather than for statistical efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_distance_mat(x1,y1,x2,y2,A,B = None):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Forms a distance matrix with ij-th entry <A(x1_i-y1_j),B(x2_i-y2_j)> where <.,.> is Euclidean inner product\n",
    "        and x_i = x[:,i] is i-th column of data, analogous for y_i. \n",
    "        If B = None then B becomes the identity.\n",
    "    Arg:\n",
    "        x1,x2: (d,n) matrix, data in columns\n",
    "        y1,y2: (d,m) matrix, data in columns\n",
    "        A: (d,d) matrix\n",
    "        B: (d,d) matrix\n",
    "    Return:\n",
    "        dist_mat: (n,m) matrix with ij-th entry <A(x1_i - y1_j), B(x2_i - y2_j)> \n",
    "                  where x1_i = x[:,i] is i-th data column, analogous for ,x2_i,y1_j,y2_j\n",
    "    \"\"\"  \n",
    "    d = x1.shape[0]\n",
    "    n = x1.shape[1]\n",
    "    m = y1.shape[1]\n",
    "    \n",
    "    if (B is None) or (B.all() is None):\n",
    "        B = np.eye(d)\n",
    "    \n",
    "    mat_x1x2 = np.einsum(\"ji,ji -> i\", A @ x1, B @ x2)\n",
    "    mat_x1x2 = np.reshape(mat_x1x2,(n,1))\n",
    "    mat_x1x2 = np.tile(mat_x1x2,(1,m))\n",
    "    \n",
    "    mat_y1y2 = np.einsum(\"ji,ji -> i\", A @ y1, B @ y2)\n",
    "    mat_y1y2 = np.reshape(mat_y1y2,(1,m))\n",
    "    mat_y1y2 = np.tile(mat_y1y2,(n,1))\n",
    "    \n",
    "    mat_x1y2 = np.einsum(\"ji,jk -> ik\",A @ x1, B @ y2)\n",
    "    mat_y1x2 = np.einsum(\"jk,ji -> ik\",A @ y1, B @ x2)\n",
    "    \n",
    "    dist_mat = mat_x1x2 + mat_y1y2 - mat_x1y2 - mat_y1x2\n",
    "    \n",
    "    return dist_mat\n",
    "\n",
    "class KSD:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Class to represent an instance of kernel Stein discrepancy.\n",
    "        Has methods to ouput the Stein kernel evaluated on data\n",
    "    \"\"\"  \n",
    "    def __init__(self,C,T,DU = 0,kernel_type = \"SE\", gamma = -1):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "            C: (d,d) matrix representing the covariance operator\n",
    "            T: (d,d) matrix representing the hyperparameter\n",
    "            DU: Function for the DU term in KSD. Default is DU = 0 which makes the DU term be 0.\n",
    "            kernel_type: either \"SE\" or \"IMQ\"\n",
    "            gamma: lengthscale, if -1 then median heuristic is employed\n",
    "        \"\"\"  \n",
    "        self.C = C\n",
    "        self.T = T\n",
    "        self.DU = DU\n",
    "        self.kernel_type = kernel_type\n",
    "        self.gamma = gamma\n",
    "        \n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "            x: (d,n) data matrix\n",
    "            y: (d,m) data matrix\n",
    "        Return:\n",
    "            Stein_mat: (n,m) matrix with ij-th entry the Stein kernel h evaluated at x_i,y_j \n",
    "        \"\"\"\n",
    "\n",
    "        n = np.shape(x)[1]\n",
    "        m = np.shape(y)[1] \n",
    "        \n",
    "        sqr_dist_mat = form_distance_mat(x,y,x,y,T,T)\n",
    "            \n",
    "        # median heuristic\n",
    "        if self.gamma == -1:\n",
    "            self.gamma = np.sqrt(np.median(sqr_dist_mat[sqr_dist_mat > 0]))\n",
    "        # changes the T which will be used later\n",
    "        self.T = self.T/self.gamma\n",
    "        # renormalises the squared distance matrix already computed that'll be used later\n",
    "        sqr_dist_mat = sqr_dist_mat/(self.gamma**2)\n",
    "        \n",
    "        # introduces variable S to make calculations easier\n",
    "        S = self.C @ np.transpose(self.T) @ self.T\n",
    "        # form the CDU terms\n",
    "        if self.DU == 0:\n",
    "            CDUx = np.zeros(np.shape(x))\n",
    "            CDUy = np.zeros(np.shape(y))\n",
    "        else:\n",
    "            CDUx = C @ self.DU(x)\n",
    "            CDUy = C @ self.DU(y)\n",
    "        \n",
    "        # <x + CDU(x),y+CDU(y)> term\n",
    "        term1 = np.einsum(\"ji,jk -> ik\",x+CDUx,y + CDUy)\n",
    "        # - <S(x-y),x-y> term\n",
    "        term2 = -1 * form_distance_mat(x,y,x,y,S)\n",
    "        # - <S(x-y),CDU(x)-CDU(y)> term\n",
    "        term3 = -1 * form_distance_mat(x,y,CDUx,CDUy,S)\n",
    "        # Tr(SC) term\n",
    "        term4 = np.trace(S @ C)\n",
    "        # ||S(x-y)||^2 term\n",
    "        term5 = -1 * form_distance_mat(x,y,x,y,S,S)\n",
    "    \n",
    "        # calculations are taken from example of Stein kernels in paper associated with SE and IMQ base kernels\n",
    "        if self.kernel_type == \"SE\":\n",
    "            \n",
    "            SE_mat = np.exp(-0.5 * sqr_dist_mat)\n",
    "            \n",
    "            Stein_mat = SE_mat * (term1 + term2 + term3 + term4 + term5)\n",
    "            \n",
    "            return Stein_mat\n",
    "        \n",
    "        if self.kernel_type == \"IMQ\":\n",
    "            \n",
    "            IMQ_mat = (sqr_dist_mat + 1)**(-0.5)\n",
    "            \n",
    "            Stein_mat = (term1 * IMQ_mat)  + ((term2 + term3 + term4) * (IMQ_mat**3)) + (3 * term5 * (IMQ_mat**5))\n",
    "            \n",
    "            return Stein_mat\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoodnessOfFitTest:\n",
    "    \"\"\"\n",
    "    Description: A single goodness-of-fit test which can produce a p-value given data and a KSD object\n",
    "    \"\"\"\n",
    "    def __init__(self, discrepancy, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            discrepancy: A callable that returns a matrix of Stein kernel evaluations\n",
    "            x: (d,n) matrix of data\n",
    "        \"\"\"\n",
    "        self.d = discrepancy\n",
    "        self.x = x\n",
    "        self.n = x.shape[1]\n",
    "        \n",
    "\n",
    "    def compute_pvalue(self, nbootstrap):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "            nbootstrap: Number of bootstrap samples.\n",
    "        Return:\n",
    "            bootstrap_stats: bootstraped test statistics\n",
    "            test_stat: the test statistic calculated using observed data\n",
    "            pvalue: p-value based on comparing test_stat with bootstrap_stats\n",
    "        \"\"\"\n",
    "        # Form the test statistic from evaluations of the Stein kernel\n",
    "        stein_matrix = self.d(self.x, self.x)\n",
    "        u_matrix = stein_matrix - np.diag(np.diag(stein_matrix))\n",
    "        test_stat = u_matrix.sum() / self.n / (self.n-1)\n",
    "        \n",
    "        # Obtain bootstrap samples using multi-nomial distribution\n",
    "        bootstrap_stats = np.zeros(n_bootstrap)\n",
    "        for i in range(n_bootstrap):\n",
    "            W = np.random.multinomial(self.n,(1./self.n)*np.ones(self.n))\n",
    "            W = (W-1)/self.n\n",
    "            bootstrap_stats[i] = W @ u_matrix @ W\n",
    "        \n",
    "        # Calculate p-value\n",
    "        pvalue = (bootstrap_stats > test_stat).mean()\n",
    "\n",
    "        return (bootstrap_stats, test_stat, pvalue)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions specify the base Gaussian measure, a Brownian bridge for this experiment, and the DU term in the KSD, which involves sin and cos functions. As discussed above the Brownian bridge basis will be used to represent our data which is why it is called in sin_DU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BB_basis(n_freqs,obs,TT):\n",
    "    \"\"\"\n",
    "        Arg:\n",
    "            n_freqs: number of basis frequencies\n",
    "            obs: observation grid to evaluate the basis elements\n",
    "            TT: time frame over which the basis is evaluated\n",
    "        Return:\n",
    "            X: (n_freqs,obs) matrix of n_freqs many basis elements each evaluated at obs locations\n",
    "    \"\"\"\n",
    "    X = np.zeros((n_freqs,len(obs)))\n",
    "    for i in range(1,n_freqs+1):\n",
    "        X[i-1,:] = (np.sqrt(2) / np.sqrt(TT)) * np.sin(i*np.pi*obs/TT)\n",
    "    return X\n",
    "\n",
    "def sin_DU(x,obs, TT = 50.0):\n",
    "    \"\"\"\n",
    "        Arg:\n",
    "            x: point to evaluate DU\n",
    "            obs: the observation grid over which to compute the inner product terms in DU\n",
    "            TT: the time frame over which the L^2 inner products are evaluated\n",
    "            random_state: random seed\n",
    "        Return:\n",
    "            DUx_freqs: (n_freqs,) array of DU evaluated at point x\n",
    "    \"\"\"\n",
    "    alpha = 0.7\n",
    "    dim = np.shape(x)[0]\n",
    "    x_recon = np.dot(BB_basis(dim,obs,TT).T,x)\n",
    "    DUx = ((alpha**2) * np.sin(x_recon)*np.cos(x_recon)) - ((alpha / 2)*np.sin(x_recon))\n",
    "    DUx_freqs = (TT/len(obs))*np.dot(BB_basis(dim,obs,TT),DUx)\n",
    "    return DUx_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two functions are used to generate our samples. The first is the standard Euler-Maruyama method foor oour non-linear SDE. The second repeatedly generates EM samples and then only acceptes those whose final value at terminal time is within the given threshld of zero.\n",
    "\n",
    "The last function then linearly interpolates the samples up to a given resolution for further processings in the KSD tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_sampler(n_samples,n_points,TT = 50,random_state = None):\n",
    "    \"\"\"\n",
    "        Arg:\n",
    "            n_samples: the number of trajectories to generate\n",
    "            n_points: the numbe of Euler-Maruyama steps to perform\n",
    "            TT: the length of time the trajectory is simulated over\n",
    "            random_state: random seed\n",
    "        Return:\n",
    "            X: (n_samples,n_points) matrix of trajectories\n",
    "    \"\"\"\n",
    "    # initialise grid for the trajectories\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    grid = np.linspace(0,TT,n_points,endpoint = True)\n",
    "    X = np.zeros((n_samples,n_points))\n",
    "    # set time step size\n",
    "    dt = grid[1]-grid[0]\n",
    "    # for each simulated point take a EM step\n",
    "    for i in range(n_points-1):\n",
    "        X[:,i+1] = X[:,i] + 0.7*np.sin(X[:,i])*dt + np.sqrt(dt)*rng.randn(n_samples)\n",
    "    return X\n",
    "\n",
    "def EM_conditional(n_samples,n_points,eps = 0.05,TT = 50,random_state = None):\n",
    "    \"\"\"\n",
    "        Arg:\n",
    "            n_samples: the number of trajectories to generate\n",
    "            n_points: the numbe of Euler-Maruyama steps to perform\n",
    "            TT: the length of time the trajectory is simulated over\n",
    "            eps: threshold within zero at time TT which the trajectories must have to be accepted\n",
    "            random_state: random seed\n",
    "        Return:\n",
    "            X: (n_samples,n_points) matrix of accepted trajectories \n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    X = np.zeros((n_samples,n_points))\n",
    "    n_accepted = 0\n",
    "    pbar = tqdm_ntb(total=n_samples)\n",
    "    # simulate trajectories until n_samples have been accepted\n",
    "    while n_accepted < n_samples:\n",
    "        # generate an EM sample\n",
    "        X_cand = EM_sampler(1,n_points,TT,None)\n",
    "        # access if EM trajectory is within threshold at zero\n",
    "        if np.abs(X_cand[0,-1]) < eps:\n",
    "            X[n_accepted,:] = X_cand\n",
    "            pbar.update(1)\n",
    "            n_accepted = n_accepted + 1\n",
    "    pbar.close()\n",
    "    return X\n",
    "\n",
    "def interpolate_samples(X,inter_grid,original_grid):\n",
    "    \"\"\"\n",
    "        Arg:\n",
    "            X: (n_samples,n_points) data matrrix\n",
    "            inter_grid: the grid up to which the data shall be linearly interpolated\n",
    "            original_grid: the grid of points the trajectories were generated over\n",
    "        Return:\n",
    "            X: (n_samples,n_points) matrix of trajectories that have been linearly interpolated up to inter_grid\n",
    "    \"\"\"\n",
    "    Y = np.zeros((len(X),len(inter_grid)))\n",
    "    # for each trajectory linearly interpolate up to inter_grid\n",
    "    for i in range(len(X)):\n",
    "        Y[i,:] = np.interp(inter_grid,original_grid,X[i,:])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters of the experiment:\n",
    "\n",
    "# Time horizon\n",
    "TT = 50\n",
    "# Number of frequencies to use of the Brownian bridge basis to represent the data\n",
    "n_freqs = 100\n",
    "# The grid the trajectories will be linearly interpolated over\n",
    "n_inter_points = 100\n",
    "inter_grid = np.linspace(0,TT,n_inter_points,endpoint = True)\n",
    "# Number of samples taken to estimate KSD\n",
    "n_samples = 2000\n",
    "# Defining a partial evaluation of the DU function \n",
    "sin_DU_partial = partial(sin_DU,obs = inter_grid, TT = TT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The covariance operator of the base Gaussian measure, in this case Brownian bridge\n",
    "C = np.diag([(1/(i*np.pi/TT))**(2) for i in np.arange(1,n_freqs + 1)])\n",
    "\n",
    "# Set hyperparameters\n",
    "T_1 = np.eye(n_freqs)\n",
    "n_adjust_freqs = 50\n",
    "T_2 = np.eye(n_freqs)\n",
    "T_2[np.diag_indices(n_adjust_freqs)] = C[np.diag_indices(n_adjust_freqs)]**(-1)\n",
    "\n",
    "# Set lengthscale\n",
    "gamma = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kernel and hyperparameters the experiment will be performed for\n",
    "kernel_list = [\"IMQ\"]\n",
    "T_list = [T_1,T_2]\n",
    "# Set range of points for the experiment\n",
    "n_points_arr = np.array([5,10,15,20,25])\n",
    "\n",
    "for (j,n_points) in enumerate(n_points_arr):\n",
    "    # Get conditioned samples\n",
    "    X = EM_conditional(n_samples,n_points,eps = 0.1)\n",
    "    # Intepolate them over inter_grid\n",
    "    inter_grid = np.linspace(0,TT,n_inter_points,endpoint=True)\n",
    "    Y = interpolate_samples(X,inter_grid,np.linspace(0,TT,n_points,endpoint=True))\n",
    "    # Calculate Brownian bridge basis representation\n",
    "    Y_freqs = (TT/n_inter_points)*np.dot(BB_basis(n_freqs,inter_grid,TT),Y.T)\n",
    "\n",
    "    for kernel in kernel_list:\n",
    "        for i in np.arange(len(T_list)):\n",
    "            T = T_list[i]\n",
    "            # Form KSD object\n",
    "            EM_KSD = KSD(C,T,DU = sin_DU_partial,kernel_type = kernel,gamma = gamma)\n",
    "            # Calculte KSD using the data\n",
    "            stein_matrix = EM_KSD(Y_freqs,Y_freqs)\n",
    "            # Calculte estimator of KSD\n",
    "            test_stat = stein_matrix.sum() / n_samples**2\n",
    "            print(\"n_points: \", n_points, \" using kernel \", kernel,\" with T_\", i+1, \"and n_samples = \", n_samples, \" has value \",test_stat)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
